{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CSVs to AWS Athena db\n",
    "\n",
    "Our sensor data is a collection of CSV of over 30 million records.  For us to analyze this huge dataset, we will load them to Athena db.  [Athena](https://aws.amazon.com/athena/) is Amazon's interactive query service to analyze data in Amazon S3 using standard SQL.  I chose Athena because:\n",
    "* **our data is already hosted in S3** - with minimal data structure we can use it directly for quary and analysis.  I also provide this data as a csv download for anyone, so the same s3 bucket can be used for sharing and distribution.\n",
    "* **its serverless** - we don't need to deploy an sql server to query and analyze.  At the same time you only gets billed whenever the data is queried. This makes it attractctive as the cost is cheaper.\n",
    "\n",
    "This guide walksthrough the basic setup of the Athena db using our weather data csv. I found a great introductory video about the basics of using Athena from [Academind](https://www.youtube.com/playlist?list=PL55RiY5tL51rZooHydslYclCYio7eoC66)\n",
    "\n",
    "*Note: We will use mostly terminal CLI and Amazon's Athena Query UI, most of these commands can be executed with AWS [Command Line Interface](https://aws.amazon.com/cli/?sc_channel=PS&sc_campaign=acquisition_IN&sc_publisher=google&sc_medium=command_line_b&sc_content=aws_cli_e&sc_detail=aws%20cli&sc_category=command_line&sc_segment=159807028048&sc_matchtype=e&sc_country=IN&s_kwcid=AL!4422!3!159807028048!e!!g!!aws%20cli&ef_id=VgvD3AAAAAAp9jGB:20180729163045:s), but for simplicity, we will use the UI fromAWS console.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "```sh\n",
    "head asg.csv \n",
    "id,name,time,value,water_level_change,time_difference,lon,lat,the_geom\n",
    "1,iloilojalaurbridgepototan_wl,2013-09-03 04:00:00,12,1.39,160,122.686106,10.909531,0101000020E6100000B8E52329E9AB5E407765170CAED12540\n",
    "2,isabelamalligbridge_rwl,2013-09-03 11:50:00,12.8,0,10,121.612111,17.1755,0101000020E610000066A19DD32C675E40B0726891ED2C3140\n",
    "3,metromanilarosariojs_wl,2013-08-25 00:10:00,12.99,-0.0299999999999994,10,121.090436,14.597919,0101000020E6100000639813B4C9455E4065506D7022322D40\n",
    "```\n",
    "\n",
    "Our raw CSV contains several columns, for example, in the automated stream gauge (asg), we have:\n",
    "\n",
    "* id - unique for each record\n",
    "* name - name of station\n",
    "* time - time of record\n",
    "* value - water level in meters\n",
    "* water_level_change - +/-  change of water level in meter\n",
    "* time_difference - time difference from previous reading\n",
    "* lon - longitude of station\n",
    "* lat - latitude of station\n",
    "* the_geom - WKB of the station location\n",
    "\n",
    "* We remove the `the_geom` column since we don't need it for Athena, the `lat` and `lon` values is enough if we need basic spatial query later.\n",
    "\n",
    "```sh\n",
    "cut -d',' -f 1-8 asg.csv > asg-clean.csv\n",
    "```\n",
    "\n",
    "Then we compress the cleaned csv.\n",
    "\n",
    "```sh\n",
    "gzip arg-clean.csv\n",
    "```\n",
    "\n",
    "Repeat for all the other 3 files.\n",
    "\n",
    "```sh\n",
    "ls -alh *.csv *.gz\n",
    "-rw-r--r--  1 maning  staff   243M Jul 28 15:13 arg-clean.csv.gz\n",
    "-rw-r--r--  1 maning  staff   1.6G Jan 21  2017 arg.csv\n",
    "-rw-r--r--  1 maning  staff   127M Jul 28 15:15 asg-clean.csv.gz\n",
    "-rw-r--r--  1 maning  staff   936M Jan 21  2017 asg.csv\n",
    "-rw-r--r--  1 maning  staff    40M Jul 28 15:17 aws-clean.csv.gz\n",
    "-rw-r--r--  1 maning  staff   337M Jan 21  2017 aws.csv\n",
    "-rw-r--r--  1 maning  staff   255M Jul 28 15:20 td-clean.csv.gz\n",
    "-rw-r--r--  1 maning  staff   1.9G Jan 21  2017 td.csv\n",
    "```\n",
    "\n",
    "Compression is very important to reduce your cost.  Amazon charged you \"based on the amount of data scanned by each query\".  Compression reduces the file size and thus reducing the cost. For example by cleaning and compressing, the previous 4.7 GB of the combined 4 files is reduced to only 665 MB or about 7x smaller.\n",
    "\n",
    "There are other data formats (i.e. Apache ORC, parquet) that can siginificantly reduce cost, but for simplicity we will use the compressed CSVs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to S3 \n",
    "\n",
    "For Athena to access the data, we need to upload the data to an s3 bucket.\n",
    "\n",
    "* Go to S3 from the [AWS Console](https://s3.console.aws.amazon.com/s3/home?region=us-east-1#)\n",
    "* Create a new bucket (for example, `s3://dostsensor-db/`)\n",
    "* Create sub directories for each compressed csv.  Each of our csv has a different data structure. A subdirectory in the main bucket can be treated as a separate table in Athena. Use the following structure for your bucket:\n",
    "\n",
    "```sh\n",
    "tree\n",
    "├──dostsensor-db\n",
    " ├── arg\n",
    " │   ├── arg-clean.csv.gz\n",
    " └── asg\n",
    " │   ├── arg-clean.csv.gz\n",
    " └── aws\n",
    " │   ├── aws-clean.csv.gz\n",
    " └── td\n",
    " │   ├── td-clean.csv.gz\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The AWS Athena UI\n",
    "\n",
    "* To access Athena, go [Athena via AWS console](https://console.aws.amazon.com/athena/home?region=us-east-1).\n",
    "* The Athena UI has the following parts:\n",
    "  1. Main query editor - where you view/write your SQL code.\n",
    "  2. List of available database and tables.\n",
    "  3. Where SQL result are displaed.\n",
    "  4. Tabs to other features of the UI.\n",
    "\n",
    "![](img/athena-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup your database and tables\n",
    "\n",
    "* From the Athena UI query window, create a new database.\n",
    "\n",
    "```\n",
    "CREATE DATABASE dostsensor;\n",
    "```\n",
    "\n",
    "Once created, the database will appear in the dropbox of the Database list.\n",
    "\n",
    "\n",
    "* Defining the table's Data Definition Language (DDL).  Each table should have a DDL, this is the schema \n",
    "that Athena will use to read the CSV.  More info about Ahena's DDLs [here](https://docs.aws.amazon.com/athena/latest/ug/ddl-sql-reference.html).\n",
    "* For the asg.csv, I used the ddl below:\n",
    "\n",
    "```sql\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS dostsensor.arg (\n",
    "  `id` int, \n",
    "  `name` string, \n",
    "  `time` string, \n",
    "  `value` decimal(10,6), \n",
    "  `lon` decimal(10,6), \n",
    "  `lat` decimal(10,6)\n",
    "  )\n",
    "ROW FORMAT DELIMITED \n",
    "  FIELDS TERMINATED BY ',' \n",
    "STORED AS INPUTFORMAT \n",
    "  'org.apache.hadoop.mapred.TextInputFormat' \n",
    "OUTPUTFORMAT \n",
    "  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION\n",
    "  's3://dostsensor-db/asg'\n",
    "TBLPROPERTIES (\n",
    "  'has_encrypted_data'='false', \n",
    "  'skip.header.line.count'='1')\n",
    "```\n",
    "\n",
    "* [todo] - explain the parts of the DDL\n",
    "* Once the table is created, it will appear in the list of tables under the `dostsensor` database\n",
    "* Do the same for all your CSVs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your database\n",
    "\n",
    "Once your tables are ready, we can test if the dat is displayed as expected.\n",
    "* In the quaery editor, type\n",
    "\n",
    "```\n",
    "SELECT * FROM \"dostsensor\".\"arg\" limit 10;\n",
    "```\n",
    "\n",
    "* The result should display in the Result window\n",
    "\n",
    "```\n",
    "|   | id       | name                                | time                | value    | lon        | lat       |\n",
    "| - | -------- | ----------------------------------- | ------------------- | -------- | ---------- | --------- |\n",
    "| 1 | 32156535 | apayaoflora_r2                      | 2015-08-29 14:15:00 | 0.000000 | 121.471109 | 18.016230 |\n",
    "| 2 | 32156538 | isabelacvaardcomplexisuechague_r1   | 2015-08-29 14:15:00 | 0.000000 | 121.683333 | 16.716667 |\n",
    "| 3 | 32156539 | agusandelsurmagsaysayprosperidad_r2 | 2015-08-29 14:30:00 | 0.000000 | 126.007080 | 8.632080  |\n",
    "| 4 | 32156540 | aklanguadalupebridgemadalag_rwl     | 2015-08-29 14:30:00 | 0.000000 | 122.309917 | 11.529853 |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all your CSV are available for query and analysis through Athena.  In the next section, we will discuss how to connect the Athena database to Jupyter notebook and pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See also\n",
    "* [AWS Athena Guide](https://docs.aws.amazon.com/athena/latest/ug/what-is.html)\n",
    "* [Athena videos by Academind](https://www.youtube.com/playlist?list=PL55RiY5tL51rZooHydslYclCYio7eoC66)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
